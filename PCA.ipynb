{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e5fae5",
   "metadata": {},
   "source": [
    "# Q1. What is the curse of dimensionality reduction and why is it important in machine learning?\n",
    "\n",
    "\n",
    "# ANS:\n",
    "\n",
    "#### Curse of dimensionality\n",
    "\n",
    "* If we provide less valued features to our model while tranning, then during tarnning pahse our model will try to learn each and every minute information of the data set and become a overfitted model which will performe bad with test data.\n",
    "\n",
    "\n",
    "\n",
    "#### Importants:\n",
    "* We need to understand this  Curse of dimensionality to find performe feature extraction or dimensionality reduction technique to help the model to become a balanced fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb12fe",
   "metadata": {},
   "source": [
    "# Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?\n",
    "\n",
    "# ANS:\n",
    " * If we provide less valued features to our model while tranning, then during tarnning pahse our model will try to learn each and every minute information of the data set and become a overfitted model which will performe bad with test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145aaaaf",
   "metadata": {},
   "source": [
    "# Q3. What are some of the consequences of the curse of dimensionality in machine learning, and how do they impact model performance?\n",
    "\n",
    "# ANS\n",
    "If we provide less valued features to our model while tranning, then during tarnning pahse our model will try to learn each and every minute information of the data set and become a overfitted model which will performe bad with test data.\n",
    "\n",
    "# Impact:\n",
    "The model will become overfitted and will performe bad with test data by returning less accuracy and other performance metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d54005",
   "metadata": {},
   "source": [
    "# Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?\n",
    "\n",
    "# ANS\n",
    "\n",
    "* In feature selection we need to find out the core important features out of the n number of the features provided with the dataset.\n",
    "* The features which are having less important we can discard them by dropping from the dataset and create the model\n",
    "* Example:\n",
    "> dataset of pen's sales cols are  cost, color shop_keeper_name, here shop_keeper_name does not having any value related to the sales of pen so we can drop the same and keep rest cols to make the model.\n",
    "* By redcuing the features we are reducing the dimension of the data to make the model optimal and balanced fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed63b78",
   "metadata": {},
   "source": [
    "# Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine learning?\n",
    "\n",
    "# ANS:\n",
    "\n",
    "### Loss of the data:\n",
    "* In feature selection we are keeping the important features and discarding the non importanten cols that cause data loss and it may have some even trival effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e5bfc",
   "metadata": {},
   "source": [
    "# Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?\n",
    "\n",
    "# ANS:\n",
    "\n",
    "If we provide less valued features to our model while tranning, then during tarnning pahse our model will try to learn each and every minute information of the data set and become a overfitted model which will performe bad with test data. The model will become overfitted and will performe bad with test data by returning less accuracy and other performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23369f",
   "metadata": {},
   "source": [
    "# Q7. How can one determine the optimal number of dimensions to reduce data to when using dimensionality reduction techniques?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "## Scree Plot or Eigenvalue Spectrum: \n",
    "* For methods like Principal Component Analysis (PCA), you can analyze the scree plot or eigenvalue spectrum. It shows the eigenvalues corresponding to each principal component. Eigenvalues represent the amount of variance explained by each component. The plot typically displays eigenvalues in descending order. You can look for an \"elbow\" or a point where the eigenvalues level off, indicating the optimal number of dimensions to retain.\n",
    "\n",
    "\n",
    "## Cumulative Explained Variance: \n",
    "* Another way to determine the optimal number of dimensions for PCA is by analyzing the cumulative explained variance plot. It shows the cumulative amount of variance explained as the number of dimensions increases. You can choose the number of dimensions that capture a significant percentage of the total variance, such as 95% or 99%.\n",
    "\n",
    "\n",
    "#### Domain Knowledge and Visualization: \n",
    "* Depending on the specific problem and domain, you may have prior knowledge about the data and the relationships between its features. If certain features are known to be less important or redundant, you can choose to reduce the dimensions accordingly. Additionally, visualizing the data in lower dimensions (e.g., 2D or 3D) can provide insights into the structure and clustering of the data, helping you determine an appropriate number of dimensions.\n",
    "\n",
    "#### Cross-validation:\n",
    "* In some cases, you can use cross-validation techniques to evaluate the performance of your model with different numbers of dimensions. By comparing the model's performance metrics (e.g., accuracy, error rate, F1 score) across different dimensions, you can select the number that gives the best trade-off between complexity and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad750e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9f933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9e9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d9302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789ca6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
